{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNMQcPa7dyhuf5JpQ9IXcyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saltycookie/RLIntroNotebook/blob/main/K_armed_Bandit_Testbed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J3VfLvtrANwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_argmax(arr, axis=None):\n",
        "  max_values_along_axis = np.max(arr, axis, keepdims=True)\n",
        "  is_max_mask = (arr == max_values_along_axis)\n",
        "  random_tiebreaker = np.where(is_max_mask, np.random.rand(*arr.shape), -1)\n",
        "  return np.argmax(random_tiebreaker, axis)\n"
      ],
      "metadata": {
        "id": "g3l_vX2ThPWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXtu31-F6uvV"
      },
      "outputs": [],
      "source": [
        "class KArmedBanditEnv:\n",
        "  def __init__(self, k, reward_std):\n",
        "    self.k = k\n",
        "    self.reward_std = reward_std\n",
        "    self.q_star = np.random.normal(0, 1, k)\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = np.random.normal(self.q_star[action], self.reward_std)\n",
        "    return reward\n",
        "\n",
        "\n",
        "class KArmedBanditAgent:\n",
        "  def __init__(self, num_runs, k, init_q=0.0, alpha=0.0):\n",
        "    self.k = k\n",
        "    self.num_runs = num_runs\n",
        "    self.q = np.zeros(shape=(num_runs, k)) + init_q\n",
        "    self.n = np.zeros(shape=(num_runs, k), dtype=int)\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def act(self):\n",
        "    pass\n",
        "\n",
        "  def update(self, action, reward):\n",
        "    selected_indices = np.arange(action.shape[0]), action\n",
        "    self.n[selected_indices] += 1\n",
        "    self.q[selected_indices] += (reward - self.q[selected_indices]) / (\n",
        "        self.alpha if self.alpha else self.n[selected_indices])\n",
        "\n",
        "\n",
        "class EpsilonGreedyAgent(KArmedBanditAgent):\n",
        "  def __init__(self, num_runs, k, init_q=0.0, alpha=0.0, epsilon=0.0):\n",
        "    super().__init__(num_runs, k, init_q, alpha)\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def act(self):\n",
        "    if self.epsilon == 0:\n",
        "      return random_argmax(self.q, axis=1)\n",
        "    is_rand_mask = np.random.rand(self.q.shape[0]) < self.epsilon\n",
        "    return np.where(is_rand_mask,\n",
        "                    np.random.randint(0, self.k, size=self.q.shape[0]),\n",
        "                    random_argmax(self.q, axis=1))\n",
        "\n",
        "\n",
        "def simulate(env, agent, num_steps):\n",
        "  rewards = []\n",
        "  for _ in range(num_steps):\n",
        "    action = agent.act()\n",
        "    reward = env.step(action)\n",
        "    agent.update(action, reward)\n",
        "    rewards.append(np.mean(reward))\n",
        "  return rewards\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = KArmedBanditEnv(10, 1)\n",
        "num_steps = 3000\n",
        "num_runs = 10000\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "greedy_agent = EpsilonGreedyAgent(num_runs, env.k)\n",
        "rewards_1 = simulate(env, greedy_agent, num_steps)\n",
        "plt.plot(rewards_1, label='Greedy')\n",
        "\n",
        "optimistic_greedy_agent = EpsilonGreedyAgent(num_runs, env.k, init_q=5)\n",
        "rewards_4 = simulate(env, optimistic_greedy_agent, num_steps)\n",
        "plt.plot(rewards_4, label='Optimistic Greedy')\n",
        "\n",
        "epsilon_greedy_agent_1 = EpsilonGreedyAgent(num_runs, env.k, epsilon=0.1)\n",
        "rewards_2 = simulate(env, epsilon_greedy_agent_1, num_steps)\n",
        "plt.plot(rewards_2, label='Espilon Greedy (ε=0.1)')\n",
        "\n",
        "epsilon_greedy_agent_2 = EpsilonGreedyAgent(num_runs, env.k, epsilon=0.01)\n",
        "rewards_3 = simulate(env, epsilon_greedy_agent_2, num_steps)\n",
        "plt.plot(rewards_3, label='Espilon Greedy (ε=0.01)')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1BC8uHLXPnWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}